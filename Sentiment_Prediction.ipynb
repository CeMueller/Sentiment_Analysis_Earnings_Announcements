{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CSVs and create concatenated df for single stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directory\n",
    "os.chdir(r'C:\\Users\\muell\\Desktop\\Thesis\\MA\\Data')\n",
    "\n",
    "# Get names of all stocks through the folder structure \n",
    "stocklist = os.listdir()\n",
    "\n",
    "# Loop to create and store (in SQL) all tweet dataframes/tables, segmented by stock ticker\n",
    "for a in range(0,len(stocklist)):\n",
    "    \n",
    "    # Define stock ticker for respective position nr in folder\n",
    "    stock = stocklist[a]\n",
    "\n",
    "    # Define path where csv files can be found\n",
    "    path = os.getcwd() + '\\\\' + stocklist[a] + '\\\\'\n",
    "\n",
    "    # Get filenames created through using multiple threads when sourcing tweets to concatenate files together later on\n",
    "    # Gets list of all files in respective path folder\n",
    "    filelist = os.listdir(path) \n",
    "    # Creates list of all files\n",
    "    all_files = glob.glob(os.path.join(path, \"*.csv\")) \n",
    "\n",
    "    # Imports all files as df, during import 0.45% to 1.64% of obs are lost due to wrong format in csv input\n",
    "    df_from_each_file = (pd.read_csv(f, sep=',',   lineterminator='\\n',\\\n",
    "                                     error_bad_lines=False, warn_bad_lines=False, quoting=csv.QUOTE_NONE) for f in all_files) \n",
    "\n",
    "    # Concatenates all df's of respective Ticker to one df\n",
    "    concatenated_df  = pd.concat(df_from_each_file, ignore_index=True, sort=False)\n",
    "\n",
    "    # Cut of excessive columns, which are added due to somehow non-dispenseable but automatic semicolons in csv file\n",
    "    df_size = 26\n",
    "    for i in range(df_size,len(concatenated_df.columns)):\n",
    "        if len(concatenated_df.columns)>df_size:\n",
    "            concatenated_df = concatenated_df.drop([concatenated_df.columns[df_size]], axis='columns')\n",
    "\n",
    "    # Remove all columns that are not required in the analysis\n",
    "    for i in [26,25,24,23,22,21,20,12,11,10,9,8,7,5,4,3,2,1,6]:\n",
    "        try:\n",
    "            concatenated_df = concatenated_df.drop([concatenated_df.columns[i]], axis='columns')\n",
    "        except: \n",
    "            continue\n",
    "\n",
    "    # Drop all rows which have nan in the last two columns, as a proxy for files with wrong data structure        \n",
    "    concatenated_df = concatenated_df.drop(concatenated_df.index[concatenated_df[\"is Retweet?\"].isnull() == True].tolist())\n",
    "    concatenated_df = concatenated_df.drop(concatenated_df.index[concatenated_df[\"is Reply?\"].isnull() == True].tolist())\n",
    "\n",
    "    # Change format of date to datetime\n",
    "    concatenated_df['Date']= pd.to_datetime(concatenated_df['Date']) \n",
    "\n",
    "    # Create connection to SQL DB, or if done for the first time: create db\n",
    "    conn = sqlite3.connect(r\"C:\\Users\\muell\\Desktop\\Thesis\\MA\\SQL_DB\\Tw_DB.db\")\n",
    "    # Save the df on SQL DB\n",
    "    concatenated_df.to_sql('{}'.format(stock), conn, if_exists='replace', index=False)\n",
    "    # Close connection to SQL DB\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "\n",
    "os.chdir(r'C:\\Users\\muell\\Desktop\\Thesis\\MA\\Data')\n",
    "stocklist = os.listdir()\n",
    "\n",
    "\n",
    "# Loop to create and store (in SQL) all tweet dataframes/tables, segmented by stock ticker\n",
    "# for a in range(0,len(stocklist)):\n",
    "\n",
    " \n",
    "for a in reversed(range(0,len(stocklist))):\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "    stock_adj = stocklist[a] + '_adj'\n",
    "    stock = stocklist[a]\n",
    "\n",
    "    conn = sqlite3.connect(r\"C:\\Users\\muell\\Desktop\\Thesis\\MA\\SQL_DB\\Tw_DB.db\")\n",
    "    c = conn.cursor()\n",
    "    # Select entire table of respective stock ticker\n",
    "    query = '''select * from {} '''.format(stock)\n",
    "    c.execute(query)\n",
    "    # Convert to pandas DF format\n",
    "    result = pd.DataFrame(c.fetchall())\n",
    "    # Rename all columns accordingly\n",
    "    result.rename(columns={0: 'ID', 1: 'Date', 2: 'Text', 3: 'N_Replies', 4: 'N_Retweets', 5: 'N_Favorites', 6: 'D_Reply', 7: 'D_Retweet'}, inplace=True)\n",
    "    # Change format of Date from object to datetime\n",
    "    # result['Date']= pd.to_datetime(result['Date']) \n",
    "    # Drop non-essential columns for now\n",
    "    result = result[['ID','Text']]\n",
    "    total_rows = result.Text.count()\n",
    "    if total_rows < 60000:\n",
    "        print('0. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "        # Converts all letters into lower case\n",
    "        result['Text'] = result['Text'].str.lower()\n",
    "\n",
    "        import re\n",
    "        # Delete all http & https URLs\n",
    "        result['Text'] = [re.sub(r\"http\\S+|rhttps\\S+|rwww\\S+\", \"\", e) for e in result['Text']]\n",
    "\n",
    "        # Delete all User Mentions\n",
    "        result['Text'] = [re.sub(r\"@\\S+\", \"\", e) for e in result['Text']]\n",
    "\n",
    "        # Remove all hashtags\n",
    "        result['Text'] = [re.sub(r'#([^\\s]+)', r'\\1', e) for e in result['Text']]\n",
    "\n",
    "        # Count number of $ in each Tweet to use as a proxy for tweet specificity (i.e. number of \n",
    "        # stock tickers referenced in each tweet) later on\n",
    "        result['N_Ticker'] = [e.count(\"$\") for e in result['Text']]\n",
    "\n",
    "        # Remove all Cashtags \n",
    "        result['Text'] = [re.sub(r\"\\$\\S+\", '', e) for e in result['Text']]\n",
    "\n",
    "        # Mitigate elongated words, by detecting positions where character is repeated 3+ times and reduced to 1 time (approximation)\n",
    "        result['Text'] = [re.sub(r'((\\w)\\2{2,})', r'\\2', e) for e in result['Text']]\n",
    "        # Apply spellchecker to correct elongated words (and increase performance of above approximation)\n",
    "        # Extremely time intense (ignore for now)\n",
    "        # from autocorrect import Speller\n",
    "        # check = Speller(lang='en')\n",
    "        print('1. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "        import nltk\n",
    "        # list of english words, used to later remove non-english words\n",
    "        words = set(nltk.corpus.words.words())\n",
    "\n",
    "        # Remove all stopwords from the tweets\n",
    "        import nltk\n",
    "        # nltk.download('punkt')\n",
    "        from nltk.corpus import stopwords\n",
    "        # nltk.download('stopwords')\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        # Tokenize the respective tweets\n",
    "        tweet_tokens = [word_tokenize(e) for e in result['Text']]\n",
    "        print('2. ' + str(timeit.default_timer() - start_time))\n",
    "        # Define the list of stopwords which will be removed\n",
    "        all_stopwords = stopwords.words('english')\n",
    "        #remove_list = []\n",
    "        # all_stopwords = [word for word in all_stopwords if word not in remove_list]\n",
    "        # Add additional stop words to be removed from texts\n",
    "        sw_list = ['amp', 'aint', \"ain't\", 'barely', 'cannot ', 'cant', \"can't\", 'couldnt', \"couldn't\", 'darent', \"daren't\", 'didnt', \\\n",
    "        \"didn't\", \"doesnt\", \"doesn't\", 'dont', \"don't\", 'hadnt', \"hadn't\", 'hardly', 'hasnt', \"hasn't\", 'havent', \\\n",
    "        \"haven't\", 'isnt', \"isn't\", 'lack', 'lacking', 'lacks', 'mightnt', \"mightn't\", 'mustnt', \"mustn't\", \\\n",
    "        'neednt', \"needn't\", 'neither', 'never', 'no one', 'nobody', 'none', 'nor',  'nothing', \\\n",
    "        'nowhere', 'oughtnt', \"oughtn't\", 'scarcely', 'shant', \"shan't\", 'shouldnt', \"shouldn't\", 'wasnt', \\\n",
    "        \"wasn't\", 'without', 'wont', \"won't\", 'wouldnt', \"wouldn't\"]\n",
    "        all_stopwords.extend(sw_list)\n",
    "\n",
    "\n",
    "        # Lemmatize the words ( preferred relative to stemming )\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        # Tokenize the respective tweets\n",
    "        tweet_tokens = [word_tokenize(e) for e in result['Text']]\n",
    "        print('3. ' + str(timeit.default_timer() - start_time))\n",
    "        for i in range(0,len(tweet_tokens)):\n",
    "            for p in ['a','s','r','n','v']:\n",
    "                tweet_tokens[i] = [lemmatizer.lemmatize(w, pos=p) for w in tweet_tokens[i]] \n",
    "            # Delete all stopwords\n",
    "            tweet_tokens[i] = [word for word in tweet_tokens[i] if word not in all_stopwords]\n",
    "\n",
    "                # join list of words back to a sentence    \n",
    "            # result.loc[i, 'Text'] = ' '.join(tweet_tokens[i])\n",
    "            # Delete all words which are non-english to reduce dimensionality of matrix later on\n",
    "            result.loc[i, 'Text'] = [\" \".join(w for w in nltk.wordpunct_tokenize(str(tweet_tokens[i])) if w in words or not w.isalpha())]\n",
    "        print('4. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "        # Negation handling:\n",
    "        # Detects negations and transforms negated words into 'not_' form\n",
    "        def negate_sequence(text):\n",
    "            negation = False\n",
    "            delims = \"?.,!:;\"\n",
    "            result = []\n",
    "            words = text.split()\n",
    "            prev = None\n",
    "            pprev = None\n",
    "            for word in words:\n",
    "                stripped = word.strip(delims).lower()\n",
    "                negated = \"not_\" + stripped if negation else stripped\n",
    "                result.append(negated)\n",
    "                prev = negated\n",
    "\n",
    "                if any(neg in word for neg in ['aint', \"ain't\", 'arent', \"aren't\" 'barely', 'cannot ', 'cant', \"can't\", 'couldnt', \"couldn't\", 'darent', \"daren't\", 'didnt', \\\n",
    "                                                \"didn't\", \"doesnt\", \"doesn't\", 'dont', \"don't\", 'hadnt', \"hadn't\", 'hardly', 'hasnt', \"hasn't\", 'havent', \\\n",
    "                                                \"haven't\", 'isnt', \"isn't\", 'lack', 'lacking', 'lacks', 'mightnt', \"mightn't\", 'mustnt', \"mustn't\", \\\n",
    "                                                'neednt', \"needn't\", 'neither', 'never', 'no', 'no one', 'nobody', 'none', 'nor', 'not', 'nothing', \\\n",
    "                                                'nowhere', 'oughtnt', \"oughtn't\", 'scarcely', 'shant', \"shan't\", 'shouldnt', \"shouldn't\", 'wasnt', \\\n",
    "                                                \"wasn't\", 'werent', \"weren't\", 'without', 'wont', \"won't\", 'wouldnt', \"wouldn't\"]):\n",
    "                    negation = not negation\n",
    "\n",
    "                if any(c in word for c in delims):\n",
    "                    negation = False\n",
    "\n",
    "            return result\n",
    "        # Application of def negate_sequence for negation handling\n",
    "        result['Text'] = [' '.join(negate_sequence(e)) for e in result['Text']]\n",
    "        print('5. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "        # Remove all punctuations and all special chars and replace with space\n",
    "        import string\n",
    "        result['Text'] = result['Text'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "        # Remove all numbers\n",
    "        result['Text'] = [re.sub(\"\\d+\", \"\", e) for e in result['Text']]\n",
    "\n",
    "        # Remove short words with 2 chars or less\n",
    "        result['Text'] = [re.sub(r'\\b\\w{1,2}\\b', '', e) for e in result['Text']]\n",
    "\n",
    "        # Replace multiple whitespaces with single whitespace\n",
    "        result['Text'] = [' '.join(str(e).split()) for e in result['Text']]\n",
    "        print('6. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "        # Save the new _adj df on SQL DB\n",
    "        result.to_sql('{}'.format(stock_adj), conn, if_exists='replace', index=False)\n",
    "        # Close connection to SQL DB\n",
    "        conn.close()\n",
    "        # code you want to evaluate\n",
    "        print('7. ' + str(timeit.default_timer() - start_time))\n",
    "        print(stock_adj)\n",
    "        print(result)\n",
    "        # result.to_csv(r'C:\\Users\\muell\\Desktop\\Thesis\\MA\\Test\\WBA.csv')\n",
    "\n",
    "    else:\n",
    "        # 20000 is the optimal size for efficiency reasons\n",
    "        N_splits = int(round(total_rows/20000))\n",
    "        result_arr = np.array_split(result,N_splits)\n",
    "        \n",
    "        def large_pre_proc(arr_num):\n",
    "            result = result_arr[arr_num]\n",
    "            \n",
    "            # to account for shift in loop below\n",
    "            sum_shift = 0\n",
    "            for ab in range(0,arr_num):\n",
    "                sum_shift = len(result_arr[ab]) + sum_shift\n",
    "\n",
    "            print('0. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "            # Converts all letters into lower case\n",
    "            result['Text'] = result['Text'].str.lower()\n",
    "\n",
    "            import re\n",
    "            # Delete all http & https URLs\n",
    "            result['Text'] = [re.sub(r\"http\\S+|rhttps\\S+|rwww\\S+\", \"\", e) for e in result['Text']]\n",
    "\n",
    "            # Delete all User Mentions\n",
    "            result['Text'] = [re.sub(r\"@\\S+\", \"\", e) for e in result['Text']]\n",
    "\n",
    "            # Remove all hashtags\n",
    "            result['Text'] = [re.sub(r'#([^\\s]+)', r'\\1', e) for e in result['Text']]\n",
    "\n",
    "            # Count number of $ in each Tweet to use as a proxy for tweet specificity (i.e. number of \n",
    "            # stock tickers referenced in each tweet) later on\n",
    "            result['N_Ticker'] = [int(e.count(\"$\")) for e in result['Text']]\n",
    "\n",
    "            # Remove all Cashtags \n",
    "            result['Text'] = [re.sub(r\"\\$\\S+\", '', e) for e in result['Text']]\n",
    "\n",
    "            # Mitigate elongated words, by detecting positions where character is repeated 3+ times and reduced to 1 time (approximation)\n",
    "            result['Text'] = [re.sub(r'((\\w)\\2{2,})', r'\\2', e) for e in result['Text']]\n",
    "            # Apply spellchecker to correct elongated words (and increase performance of above approximation)\n",
    "            # Extremely time intense (ignore for now)\n",
    "            # from autocorrect import Speller\n",
    "            # check = Speller(lang='en')\n",
    "            print('1. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "            import nltk\n",
    "            # list of english words, used to later remove non-english words\n",
    "            words = set(nltk.corpus.words.words())\n",
    "\n",
    "                       # Remove all stopwords from the tweets\n",
    "            import nltk\n",
    "            # nltk.download('punkt')\n",
    "            from nltk.corpus import stopwords\n",
    "            # nltk.download('stopwords')\n",
    "            print('2. ' + str(timeit.default_timer() - start_time))\n",
    "            # Define the list of stopwords which will be removed\n",
    "            all_stopwords = stopwords.words('english')\n",
    "            #remove_list = []\n",
    "            # all_stopwords = [word for word in all_stopwords if word not in remove_list]\n",
    "            # Add additional stop words to be removed from texts\n",
    "            sw_list = ['amp', 'aint', \"ain't\", 'barely', 'cannot ', 'cant', \"can't\", 'couldnt', \"couldn't\", 'darent', \"daren't\", 'didnt', \\\n",
    "            \"didn't\", \"doesnt\", \"doesn't\", 'dont', \"don't\", 'hadnt', \"hadn't\", 'hardly', 'hasnt', \"hasn't\", 'havent', \\\n",
    "            \"haven't\", 'isnt', \"isn't\", 'lack', 'lacking', 'lacks', 'mightnt', \"mightn't\", 'mustnt', \"mustn't\", \\\n",
    "            'neednt', \"needn't\", 'neither', 'never', 'no one', 'nobody', 'none', 'nor',  'nothing', \\\n",
    "            'nowhere', 'oughtnt', \"oughtn't\", 'scarcely', 'shant', \"shan't\", 'shouldnt', \"shouldn't\", 'wasnt', \\\n",
    "            \"wasn't\", 'without', 'wont', \"won't\", 'wouldnt', \"wouldn't\"]\n",
    "            all_stopwords.extend(sw_list)\n",
    "\n",
    "\n",
    "            # Lemmatize the words ( preferred relative to stemming )\n",
    "            from nltk.stem import WordNetLemmatizer\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            # Tokenize the respective tweets\n",
    "            from nltk.tokenize import word_tokenize\n",
    "            tweet_tokens = [word_tokenize(e) for e in result['Text']]\n",
    "            print('3. ' + str(timeit.default_timer() - start_time))\n",
    "            for i in range(0,len(tweet_tokens)):\n",
    "                for p in ['a','s','r','n','v']:\n",
    "                    tweet_tokens[i] = [lemmatizer.lemmatize(w, pos=p) for w in tweet_tokens[i]] \n",
    "                # Delete all stopwords\n",
    "                tweet_tokens[i] = [word for word in tweet_tokens[i] if word not in all_stopwords]\n",
    "\n",
    "                    # join list of words back to a sentence    \n",
    "                # result.loc[i, 'Text'] = ' '.join(tweet_tokens[i])\n",
    "                # Delete all words which are non-english to reduce dimensionality of matrix later on\n",
    "                result.loc[(i+sum_shift), 'Text'] = [\" \".join(w for w in nltk.wordpunct_tokenize(str(tweet_tokens[i])) if w in words or not w.isalpha())]\n",
    "            print('4. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "            # Negation handling:\n",
    "            # Detects negations and transforms negated words into 'not_' form\n",
    "            def negate_sequence(text):\n",
    "                negation = False\n",
    "                delims = \"?.,!:;\"\n",
    "                result = []\n",
    "                words = text.split()\n",
    "                prev = None\n",
    "                pprev = None\n",
    "                for word in words:\n",
    "                    stripped = word.strip(delims).lower()\n",
    "                    negated = \"not_\" + stripped if negation else stripped\n",
    "                    result.append(negated)\n",
    "                    prev = negated\n",
    "\n",
    "                    if any(neg in word for neg in ['aint', \"ain't\", 'arent', \"aren't\" 'barely', 'cannot ', 'cant', \"can't\", 'couldnt', \"couldn't\", 'darent', \"daren't\", 'didnt', \\\n",
    "                                                    \"didn't\", \"doesnt\", \"doesn't\", 'dont', \"don't\", 'hadnt', \"hadn't\", 'hardly', 'hasnt', \"hasn't\", 'havent', \\\n",
    "                                                    \"haven't\", 'isnt', \"isn't\", 'lack', 'lacking', 'lacks', 'mightnt', \"mightn't\", 'mustnt', \"mustn't\", \\\n",
    "                                                    'neednt', \"needn't\", 'neither', 'never', 'no', 'no one', 'nobody', 'none', 'nor', 'not', 'nothing', \\\n",
    "                                                    'nowhere', 'oughtnt', \"oughtn't\", 'scarcely', 'shant', \"shan't\", 'shouldnt', \"shouldn't\", 'wasnt', \\\n",
    "                                                    \"wasn't\", 'werent', \"weren't\", 'without', 'wont', \"won't\", 'wouldnt', \"wouldn't\"]):\n",
    "                        negation = not negation\n",
    "\n",
    "                    if any(c in word for c in delims):\n",
    "                        negation = False\n",
    "\n",
    "                return result\n",
    "            # Application of def negate_sequence for negation handling\n",
    "            result['Text'] = [' '.join(negate_sequence(e)) for e in result['Text']]\n",
    "            print('5. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "            # Remove all punctuations and all special chars and replace with space\n",
    "            import string\n",
    "            result['Text'] = result['Text'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "            # Remove all numbers\n",
    "            result['Text'] = [re.sub(\"\\d+\", \"\", e) for e in result['Text']]\n",
    "\n",
    "            # Remove short words with 2 chars or less\n",
    "            result['Text'] = [re.sub(r'\\b\\w{1,2}\\b', '', e) for e in result['Text']]\n",
    "\n",
    "            # Replace multiple whitespaces with single whitespace\n",
    "            result['Text'] = [' '.join(str(e).split()) for e in result['Text']]\n",
    "            print('6. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "            return result\n",
    "\n",
    "        final_result = list(map(large_pre_proc,list(range(0, (N_splits)))))\n",
    "        \n",
    "        # Merge file\n",
    "        new_final_result = pd.DataFrame(final_result[0])\n",
    "        for x in range (1,N_splits):\n",
    "            new_final_result = new_final_result.append(pd.DataFrame(final_result[x]), ignore_index=True) \n",
    "        print(new_final_result)\n",
    "        # Save the new _adj df on SQL DB\n",
    "        new_final_result.to_sql('{}'.format(stock_adj), conn, if_exists='replace', index=False)\n",
    "        # Close connection to SQL DB\n",
    "        conn.close()\n",
    "        # code you want to evaluate\n",
    "        print('7. ' + str(timeit.default_timer() - start_time))\n",
    "        print(stock_adj)\n",
    "\n",
    "        # result.to_csv(r'C:\\Users\\muell\\Desktop\\Thesis\\MA\\Test\\WBA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retry with better negation handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 0.2031680000363849\n",
      "1. 0.6521559000248089\n",
      "3. 3.547253600030672\n",
      "4. 24.208250700030476\n",
      "5. 26.513613100047223\n",
      "[' short ' ' sale ' ' volume ' '(' ' not not_' ' short ' ' interest ' ')' ' for ' ' at ' ' 2019 - 12 - 27 ' ' be ' ' 44 ' '%' '.' ' 48 ' '%' ' 72 ' '%' ' 36 ' '%' ' 51 ' '%']\n",
      "6. 30.16239690000657\n",
      "7. 48.0102034000447\n",
      "short sale volume not_ short interest\n",
      "0. 48.05067650001729\n",
      "1. 48.36541590001434\n",
      "3. 50.79544120002538\n",
      "4. 72.69716740003787\n",
      "5. 74.53657140000723\n",
      "[' ' ' group ' ' ' ' sell ' ' 49 ' ';' ' 263 ' ' share ' ' of ' ' ' ' instrument ' ' incorporate ']\n",
      "6. 77.64345710002817\n",
      "7. 95.72364490001928\n",
      "group sell share instrument incorporate\n",
      "                        ID                                               Text  \\\n",
      "0      1212153219473784833  plan head refuse enter stop loss order year ag...   \n",
      "1      1212138180738256897                               best nasdaq heat map   \n",
      "2      1212116817583337473  day move average cross day move average view o...   \n",
      "3      1212116312597630976                                         inside day   \n",
      "4      1212111784670351361  analyst expect instrument incorporate post qua...   \n",
      "...                    ...                                                ...   \n",
      "33548   682899849189175296                   company small gap fill bank corp   \n",
      "33549   682896001368195072                                stock guru keep buy   \n",
      "33550   682799089793265664       blue chip dividend stock likely perform well   \n",
      "33551   682759611603107840                          big loser today large cap   \n",
      "33552   682755259567976448                            instrument high finance   \n",
      "\n",
      "       N_Ticker  \n",
      "0             1  \n",
      "1            26  \n",
      "2             1  \n",
      "3            32  \n",
      "4             2  \n",
      "...         ...  \n",
      "33548         2  \n",
      "33549         1  \n",
      "33550         8  \n",
      "33551         8  \n",
      "33552         3  \n",
      "\n",
      "[33553 rows x 3 columns]\n",
      "8. 95.7665590000106\n",
      "TXN_adj\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "\n",
    "os.chdir(r'C:\\Users\\muell\\Desktop\\Thesis\\MA\\Data')\n",
    "stocklist = os.listdir()\n",
    "\n",
    "\n",
    "# Loop to create and store (in SQL) all tweet dataframes/tables, segmented by stock ticker\n",
    "# for a in range(0,len(stocklist)):\n",
    "\n",
    " \n",
    "for a in reversed(range(0,len(stocklist))):\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "    stock_adj = stocklist[a] + '_adj'\n",
    "    stock = stocklist[a]\n",
    "\n",
    "    conn = sqlite3.connect(r\"C:\\Users\\muell\\Desktop\\Thesis\\MA\\SQL_DB\\Tw_DB.db\")\n",
    "    c = conn.cursor()\n",
    "    # Select entire table of respective stock ticker\n",
    "    query = '''select * from {} '''.format(stock)\n",
    "    c.execute(query)\n",
    "    # Convert to pandas DF format\n",
    "    result = pd.DataFrame(c.fetchall())\n",
    "    # Rename all columns accordingly\n",
    "    result.rename(columns={0: 'ID', 1: 'Date', 2: 'Text', 3: 'N_Replies', 4: 'N_Retweets', 5: 'N_Favorites', 6: 'D_Reply', 7: 'D_Retweet'}, inplace=True)\n",
    "    # Change format of Date from object to datetime\n",
    "    # result['Date']= pd.to_datetime(result['Date']) \n",
    "    # Drop non-essential columns for now\n",
    "    result = result[['ID','Text']]\n",
    "    total_rows = result.Text.count()\n",
    "    # 20000 is the optimal size for efficiency reasons\n",
    "    N_splits = int(round(total_rows/20000))\n",
    "    result_arr = np.array_split(result,N_splits)\n",
    "\n",
    "    def large_pre_proc(arr_num):\n",
    "        result = result_arr[arr_num]\n",
    "\n",
    "        # to account for shift in loop below\n",
    "        sum_shift = 0\n",
    "        for ab in range(0,arr_num):\n",
    "            sum_shift = len(result_arr[ab]) + sum_shift\n",
    "        print('0. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "        # Converts all letters into lower case\n",
    "        result['Text'] = result['Text'].str.lower()\n",
    "\n",
    "        import re\n",
    "        # Delete all http & https URLs\n",
    "        result['Text'] = [re.sub(r\"http\\S+|rhttps\\S+|rwww\\S+\", \"\", e) for e in result['Text']]\n",
    "\n",
    "        # Delete all User Mentions\n",
    "        result['Text'] = [re.sub(r\"@\\S+\", \"\", e) for e in result['Text']]\n",
    "\n",
    "        # Remove all hashtags\n",
    "        result['Text'] = [re.sub(r'#([^\\s]+)', r'\\1', e) for e in result['Text']]\n",
    "\n",
    "        # Count number of $ in each Tweet to use as a proxy for tweet specificity (i.e. number of \n",
    "        # stock tickers referenced in each tweet) later on\n",
    "        result['N_Ticker'] = [int(e.count(\"$\")) for e in result['Text']]\n",
    "\n",
    "        # Remove all Cashtags \n",
    "        result['Text'] = [re.sub(r\"\\$\\S+\", '', e) for e in result['Text']]\n",
    "\n",
    "        # Mitigate elongated words, by detecting positions where character is repeated 3+ times and reduced to 1 time (approximation)\n",
    "        result['Text'] = [re.sub(r'((\\w)\\2{2,})', r'\\2', e) for e in result['Text']]\n",
    "        # Apply spellchecker to correct elongated words (and increase performance of above approximation)\n",
    "        # Extremely time intense (ignore for now)\n",
    "        # from autocorrect import Speller\n",
    "        # check = Speller(lang='en')\n",
    "        print('1. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "        import nltk\n",
    "        # list of english words, used to later remove non-english words\n",
    "        words = set(nltk.corpus.words.words())\n",
    "\n",
    "        # nltk.download('punkt')\n",
    "        from nltk.corpus import stopwords\n",
    "        # nltk.download('stopwords')\n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        # Lemmatize the words ( preferred relative to stemming )\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        # Tokenize the respective tweets\n",
    "        tweet_tokens = [word_tokenize(e) for e in result['Text']]\n",
    "        print('3. ' + str(timeit.default_timer() - start_time))\n",
    "        for i in range(0,len(tweet_tokens)):\n",
    "            for p in ['a','s','r','n','v']:\n",
    "                tweet_tokens[i] = [lemmatizer.lemmatize(w, pos=p) for w in tweet_tokens[i]] \n",
    "                # join list of words back to a sentence    \n",
    "            # result.loc[i, 'Text'] = ' '.join(tweet_tokens[i])\n",
    "            # Delete all words which are non-english to reduce dimensionality of matrix later on\n",
    "            result.loc[(i+sum_shift), 'Text'] = [\" \".join(w for w in nltk.wordpunct_tokenize(str(tweet_tokens[i])) if w in words or not w.isalpha())]\n",
    "        print('4. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "        # Negation handling:\n",
    "        # Detects negations and transforms negated words into 'not_' form\n",
    "        def negate_sequence(text):\n",
    "            negation = False\n",
    "            delims = \"?.,!:;\"\n",
    "            result = []\n",
    "            words = text.split()\n",
    "            prev = None\n",
    "            pprev = None\n",
    "            for word in words:\n",
    "                stripped = word.strip(delims).lower()\n",
    "                negated = \"not_\" + stripped if negation else stripped\n",
    "                result.append(negated)\n",
    "                prev = negated\n",
    "\n",
    "                if any(neg in word for neg in [ 'barely', 'cannot',  'hardly', 'lack', 'lacking', 'lacks', 'neither', 'never', \\\n",
    "                                               'no one', 'nobody', 'none',  'nothing', 'nowhere',  'scarcely', \\\n",
    "                                               'without', ' not ', \"n't\", \" no \" ]):\n",
    "                    negation = not negation\n",
    "\n",
    "                if any(c in word for c in delims):\n",
    "                    negation = False\n",
    "\n",
    "            return result\n",
    "        # Application of def negate_sequence for negation handling\n",
    "        result_temp = result['Text'].apply(negate_sequence)            \n",
    "        result[\"Text\"] = list(\" \".join(e) for e in result_temp)\n",
    "        print('5. ' + str(timeit.default_timer() - start_time))\n",
    "        # Remove stopwords\n",
    "        # Tokenize the respective tweets\n",
    "        tweet_tokens = [word_tokenize(e) for e in result['Text']]\n",
    "        print('6. ' + str(timeit.default_timer() - start_time))\n",
    "        # Define the list of stopwords which will be removed\n",
    "        all_stopwords = stopwords.words('english')\n",
    "        #remove_list = []\n",
    "        # all_stopwords = [word for word in all_stopwords if word not in remove_list]\n",
    "        # Add additional stop words to be removed from texts\n",
    "        sw_list = ['amp', 'aint', \"ain't\", 'barely', 'cannot ', 'cant', \"can't\", 'couldnt', \"couldn't\", 'darent', \"daren't\", 'didnt', \\\n",
    "        \"didn't\", \"doesnt\", \"doesn't\", 'dont', \"don't\", 'hadnt', \"hadn't\", 'hardly', 'hasnt', \"hasn't\", 'havent', \\\n",
    "        \"haven't\", 'isnt', \"isn't\", 'lack', 'lacking', 'lacks', 'mightnt', \"mightn't\", 'mustnt', \"mustn't\", \\\n",
    "        'neednt', \"needn't\", 'neither', 'never', 'no one', 'nobody', 'none', 'nor',  'nothing', \\\n",
    "        'nowhere', 'oughtnt', \"oughtn't\", 'scarcely', 'shant', \"shan't\", 'shouldnt', \"shouldn't\", 'wasnt', \\\n",
    "        \"wasn't\", 'without', 'wont', \"won't\", 'wouldnt', \"wouldn't\"]\n",
    "        all_stopwords.extend(sw_list)\n",
    "#        remove_list = ['not']\n",
    "#        all_stopwords = [word for word in all_stopwords if word not in remove_list]\n",
    "        for i in range(0,len(tweet_tokens)):        \n",
    "            # Delete all stopwords\n",
    "            result.loc[(i+sum_shift), 'Text'] = [\" \".join(word for word in tweet_tokens[i] if word not in all_stopwords)]        \n",
    "\n",
    "        # Remove all punctuations and all special chars and replace with space\n",
    "        import string\n",
    "        result['Text'] = result['Text'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "        # Remove all numbers\n",
    "        result['Text'] = [re.sub(\"\\d+\", \"\", e) for e in result['Text']]\n",
    "\n",
    "        # Remove short words with 2 chars or less\n",
    "        result['Text'] = [re.sub(r'\\b\\w{1,2}\\b', '', e) for e in result['Text']]\n",
    "\n",
    "        # Replace multiple whitespaces with single whitespace\n",
    "        result['Text'] = [' '.join(str(e).split()) for e in result['Text']]\n",
    "        print('7. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "        return result\n",
    "\n",
    "    final_result = list(map(large_pre_proc,list(range(0, (N_splits)))))\n",
    "\n",
    "    # Merge file\n",
    "    new_final_result = pd.DataFrame(final_result[0])\n",
    "    for x in range (1,(N_splits)):\n",
    "        new_final_result = new_final_result.append(pd.DataFrame(final_result[x]), ignore_index=True) \n",
    "    print(new_final_result)\n",
    "    # Save the new _adj df on SQL DB\n",
    "    new_final_result.to_sql('{}'.format(stock_adj), conn, if_exists='replace', index=False)\n",
    "    # Close connection to SQL DB\n",
    "    conn.close()\n",
    "    # code you want to evaluate\n",
    "    print('8. ' + str(timeit.default_timer() - start_time))\n",
    "    print(stock_adj)\n",
    "\n",
    "# new_final_result.to_csv(r'C:\\Users\\muell\\Desktop\\Thesis\\MA\\Test\\WBA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing  annotated Tweets (for lexicon extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 7.459310699996422\n",
      "1. 8.0694033999971\n",
      "3. 15.538733900000807\n",
      "4. 54.21997209999972\n",
      "5. 56.32498209999903\n",
      "6. 61.11991829999897\n",
      "7. 84.62575239999569\n",
      "0. 84.6758417999954\n",
      "1. 84.9717845999985\n",
      "3. 88.38962049999827\n",
      "4. 113.75850910000008\n",
      "5. 116.27590280000004\n",
      "6. 121.1004352999953\n",
      "7. 144.20949459999974\n",
      "0. 144.25879829999758\n",
      "1. 144.53136829999858\n",
      "3. 148.1999066999997\n",
      "4. 174.07531200000085\n",
      "5. 176.120917799999\n",
      "6. 181.46306470000127\n",
      "7. 204.74822599999607\n",
      "0. 204.7980133000019\n",
      "1. 205.076527600002\n",
      "3. 208.40280319999874\n",
      "4. 234.19747110000026\n",
      "5. 236.36134080000193\n",
      "6. 241.35615400000097\n",
      "7. 265.730974099999\n",
      "0. 265.78226449999784\n",
      "1. 266.0647847\n",
      "3. 269.46988549999514\n",
      "4. 296.0691097000017\n",
      "5. 298.2642524999974\n",
      "6. 303.09060099999624\n",
      "7. 326.9310323999962\n",
      "0. 326.9835979999989\n",
      "1. 327.2572607999973\n",
      "3. 330.7683784999972\n",
      "4. 356.2809283999959\n",
      "5. 358.37681449999945\n",
      "6. 363.1714292999968\n",
      "7. 385.9909817000007\n",
      "0. 386.0403567999965\n",
      "1. 386.31663639999897\n",
      "3. 389.8802280000018\n",
      "4. 416.2317363999973\n",
      "5. 418.3514765\n",
      "6. 423.66273640000145\n",
      "7. 446.8890945999956\n",
      "0. 446.94198830000096\n",
      "1. 447.2431125000003\n",
      "3. 451.0841730999964\n",
      "4. 480.5646564999988\n",
      "5. 482.99475629999506\n",
      "6. 488.54754700000194\n",
      "7. 512.7521519000002\n",
      "0. 512.8033872000015\n",
      "1. 513.0726482999962\n",
      "3. 516.8100340999954\n",
      "4. 542.1593645999965\n",
      "5. 544.3433913999979\n",
      "6. 549.3970264999953\n",
      "7. 572.4886072000008\n",
      "0. 572.5385830000014\n",
      "1. 572.8036100999961\n",
      "3. 576.1521578999964\n",
      "4. 602.1060922999968\n",
      "5. 604.4139773999996\n",
      "6. 609.2876170000018\n",
      "7. 633.0869296000019\n",
      "0. 633.1400393000004\n",
      "1. 633.444957300002\n",
      "3. 637.1373457000009\n",
      "4. 664.7774491000018\n",
      "5. 666.859583999998\n",
      "6. 671.7473553999953\n",
      "7. 697.1757018999997\n",
      "0. 697.2251112000013\n",
      "1. 697.4916197999992\n",
      "3. 701.0490532999975\n",
      "4. 728.6194227000015\n",
      "5. 730.8231285000002\n",
      "6. 735.9324172999986\n",
      "7. 760.5219585000013\n",
      "0. 760.5762012999985\n",
      "1. 760.8606818999979\n",
      "3. 764.3541427999953\n",
      "4. 792.3903633000009\n",
      "5. 794.5641134999969\n",
      "6. 799.4597625999959\n",
      "7. 823.8363658000017\n",
      "0. 823.8885549999977\n",
      "1. 824.2011340999961\n",
      "3. 827.6414167999974\n",
      "4. 855.7731980999961\n",
      "5. 858.1176995999995\n",
      "6. 863.0453684000022\n",
      "7. 887.6180619000006\n",
      "0. 887.6675484000007\n",
      "1. 887.9659195999993\n",
      "3. 891.3778812999954\n",
      "4. 921.6245273000022\n",
      "5. 923.8391616000008\n",
      "6. 929.9333198000022\n",
      "7. 960.3396352999989\n",
      "0. 960.391266599996\n",
      "1. 960.6953914999976\n",
      "3. 964.6548416000005\n",
      "4. 991.1523134999952\n",
      "5. 993.2650231000007\n",
      "6. 998.4168434999956\n",
      "7. 1031.1215266\n",
      "0. 1031.1783694999976\n",
      "1. 1031.4566436999958\n",
      "3. 1035.1719716000007\n",
      "4. 1062.1129193000015\n",
      "5. 1064.2417967000001\n",
      "6. 1069.1556095999986\n",
      "7. 1093.0844109999962\n",
      "0. 1093.1372476999968\n",
      "1. 1093.4117492000005\n",
      "3. 1096.8638871000003\n",
      "4. 1122.3981835000013\n",
      "5. 1124.7662122999973\n",
      "6. 1129.7774916999988\n",
      "7. 1153.7111817999976\n",
      "0. 1153.7606928999958\n",
      "1. 1154.0324527999983\n",
      "3. 1157.663695999996\n",
      "4. 1201.1424849999967\n",
      "5. 1204.390967200001\n",
      "6. 1210.6171587999997\n",
      "7. 1243.495001999996\n",
      "0. 1243.5452728999953\n",
      "1. 1243.851946499999\n",
      "3. 1247.8242475000006\n",
      "4. 1283.1489165999956\n",
      "5. 1285.8876094000007\n",
      "6. 1291.3605873999986\n",
      "7. 1320.2731797000015\n",
      "0. 1320.3240447999997\n",
      "1. 1320.6086677999992\n",
      "3. 1324.4337199000001\n",
      "4. 1359.764688700001\n",
      "5. 1363.145528000001\n",
      "6. 1369.408842799996\n",
      "7. 1396.917547500001\n",
      "0. 1396.9667925999966\n",
      "1. 1397.2385337000014\n",
      "3. 1400.849033999999\n",
      "4. 1429.0388048000023\n",
      "5. 1431.992551299998\n",
      "6. 1436.9920242000007\n",
      "7. 1466.1263345999978\n",
      "0. 1466.1805802999952\n",
      "1. 1466.501309899999\n",
      "3. 1470.2714699999997\n",
      "4. 1500.861422199996\n",
      "5. 1502.9673512000008\n",
      "6. 1508.418829299997\n",
      "7. 1535.3170150000005\n",
      "0. 1535.3651639999953\n",
      "1. 1535.6628216999961\n",
      "3. 1539.9285669999954\n",
      "4. 1570.7111164000016\n",
      "5. 1573.415938500002\n",
      "6. 1578.4243120999963\n",
      "7. 1607.5120438000013\n",
      "0. 1607.5628818999976\n",
      "1. 1607.853685199996\n",
      "3. 1611.6978660000023\n",
      "4. 1651.5000977000018\n",
      "5. 1654.577069400002\n",
      "6. 1660.2434940999956\n",
      "7. 1690.4528329000022\n",
      "0. 1690.5044153999988\n",
      "1. 1690.8274061999982\n",
      "3. 1694.8502685999993\n",
      "4. 1729.478757799996\n",
      "5. 1732.1416220999963\n",
      "6. 1738.0112653999968\n",
      "7. 1769.681658900001\n",
      "0. 1769.7517518000022\n",
      "1. 1770.0939891000016\n",
      "3. 1775.0946982999958\n",
      "4. 1812.3857295999987\n",
      "5. 1815.229424899997\n",
      "6. 1820.8387483999977\n",
      "7. 1849.895821799997\n",
      "0. 1849.9468034999954\n",
      "1. 1850.2477790999983\n",
      "3. 1853.9646988999957\n",
      "4. 1881.7370742999992\n",
      "5. 1883.8532474000021\n",
      "6. 1888.644850699995\n",
      "7. 1912.0499956999993\n",
      "0. 1912.100335199997\n",
      "1. 1912.3701606999966\n",
      "3. 1915.6866823000018\n",
      "4. 1941.2064243999994\n",
      "5. 1943.4891097\n",
      "6. 1948.2670055999988\n",
      "7. 1971.8820412999994\n",
      "0. 1971.9392651000016\n",
      "1. 1972.319015000001\n",
      "3. 1975.7613706999982\n",
      "4. 2001.090105700001\n",
      "5. 2003.1884005999964\n",
      "6. 2008.2191426999998\n",
      "7. 2031.2321668000004\n",
      "0. 2031.2820282999965\n",
      "1. 2031.5674307000008\n",
      "3. 2034.9483332000018\n",
      "4. 2064.452633599998\n",
      "5. 2066.6857057000016\n",
      "6. 2071.715590599997\n",
      "7. 2095.5851110000003\n",
      "0. 2095.635725799999\n",
      "1. 2095.9023104000007\n",
      "3. 2099.326026399998\n",
      "4. 2127.043335999995\n",
      "5. 2129.149933200002\n",
      "6. 2134.4006579999987\n",
      "7. 2157.7594477999955\n",
      "0. 2157.8093452999965\n",
      "1. 2158.088654799998\n",
      "3. 2161.406822799996\n",
      "4. 2187.2381551\n",
      "5. 2189.440639100001\n",
      "6. 2194.354695199996\n",
      "7. 2218.7128940999974\n",
      "0. 2218.762875799999\n",
      "1. 2219.0346150999976\n",
      "3. 2222.3183709000004\n",
      "4. 2248.108184799996\n",
      "5. 2250.306511900002\n",
      "6. 2255.1440050999954\n",
      "7. 2278.8008063000016\n",
      "0. 2278.872026500001\n",
      "1. 2279.1662904999976\n",
      "3. 2282.4846925999955\n",
      "4. 2308.7214853999976\n",
      "5. 2310.8349180000005\n",
      "6. 2315.64228\n",
      "7. 2338.534076099997\n",
      "0. 2338.583384599995\n",
      "1. 2338.8586871000007\n",
      "3. 2342.431262099999\n",
      "4. 2367.846497599996\n",
      "5. 2370.132193199999\n",
      "6. 2375.199642200001\n",
      "7. 2398.544142699997\n",
      "0. 2398.5932785999976\n",
      "1. 2398.873792999999\n",
      "3. 2402.3225342000005\n",
      "4. 2428.1761780000015\n",
      "5. 2430.3171109999967\n",
      "6. 2435.1819401999965\n",
      "7. 2459.0778498\n",
      "0. 2459.1265147000013\n",
      "1. 2459.414499300001\n",
      "3. 2462.8008687999973\n",
      "4. 2488.6478644000017\n",
      "5. 2490.6942255999966\n",
      "6. 2495.402495299997\n",
      "7. 2518.8997084000002\n",
      "0. 2518.9712556000013\n",
      "1. 2519.282131300002\n",
      "3. 2522.5653760999994\n",
      "4. 2547.6513198999965\n",
      "5. 2550.247529299995\n",
      "6. 2555.058012399997\n",
      "7. 2578.2489961999963\n",
      "0. 2578.2989413999967\n",
      "1. 2578.578925599999\n",
      "3. 2582.265520699999\n",
      "4. 2607.5327892999994\n",
      "5. 2609.715692400001\n",
      "6. 2614.8922466000004\n",
      "7. 2637.9440057999964\n",
      "0. 2637.9930842999966\n",
      "1. 2638.2753873999973\n",
      "3. 2641.577814600001\n",
      "4. 2666.795234099998\n",
      "5. 2668.879959099999\n",
      "6. 2673.5890322999985\n",
      "7. 2696.432342399996\n",
      "0. 2696.4779655999955\n",
      "1. 2696.7474946\n",
      "3. 2700.091076500001\n",
      "4. 2725.1819512\n",
      "5. 2727.3320153000022\n",
      "6. 2732.0241379\n",
      "7. 2754.1143005999984\n",
      "0. 2754.159126300001\n",
      "1. 2754.431907799997\n",
      "3. 2758.1444077999986\n",
      "4. 2782.8185584999956\n",
      "5. 2784.7984995000006\n",
      "6. 2789.7540735000002\n",
      "7. 2812.3570181999967\n",
      "0. 2812.4031202999977\n",
      "1. 2812.6849251999956\n",
      "3. 2816.063407499998\n",
      "4. 2841.104942099999\n",
      "5. 2843.1356596999976\n",
      "6. 2847.952522300002\n",
      "7. 2870.7713724999994\n",
      "0. 2870.817034599997\n",
      "1. 2871.180462700002\n",
      "3. 2874.6681937999965\n",
      "4. 2899.0964927999958\n",
      "5. 2901.4180003\n",
      "6. 2906.1642365000007\n",
      "7. 2928.4084809999986\n",
      "0. 2928.453807699996\n",
      "1. 2928.7387864999982\n",
      "3. 2932.334008500002\n",
      "4. 2957.557339799998\n",
      "5. 2959.6069067999997\n",
      "6. 2964.495076899999\n",
      "7. 2986.6204015999974\n",
      "0. 2986.665683399995\n",
      "1. 2986.9496182000003\n",
      "3. 2990.433295900002\n",
      "4. 3016.1389796000003\n",
      "5. 3018.1479198000015\n",
      "6. 3023.0373985999977\n",
      "7. 3045.576868199998\n",
      "0. 3045.6221492999975\n",
      "1. 3045.8937946000005\n",
      "3. 3049.6280277999977\n",
      "4. 3073.9302410999953\n",
      "5. 3076.1025212000022\n",
      "6. 3081.1520557999975\n",
      "7. 3103.072307299997\n",
      "0. 3103.118054999999\n",
      "1. 3103.3876277000018\n",
      "3. 3106.840538999997\n",
      "4. 3131.683396399996\n",
      "5. 3133.7264856999973\n",
      "6. 3138.589470400002\n",
      "7. 3161.328991399998\n",
      "0. 3161.3732869000014\n",
      "1. 3161.649765000002\n",
      "3. 3164.965524799998\n",
      "4. 3190.305851899997\n",
      "5. 3192.3405367000014\n",
      "6. 3197.069875399997\n",
      "7. 3219.5822215000007\n",
      "0. 3219.6290856999985\n",
      "1. 3219.912733799996\n",
      "3. 3223.6729901\n",
      "4. 3247.8941628999964\n",
      "5. 3249.9975066999978\n",
      "6. 3255.292067399998\n",
      "7. 3277.5672750000012\n",
      "0. 3277.6122106999974\n",
      "1. 3277.885632999998\n",
      "3. 3281.262476899996\n",
      "4. 3306.125486099998\n",
      "5. 3308.0671937000006\n",
      "6. 3312.8154926999996\n",
      "7. 3335.0511184999996\n",
      "0. 3335.102785999996\n",
      "1. 3335.368483899998\n",
      "3. 3338.7981340999977\n",
      "4. 3362.7869689\n",
      "5. 3364.8914317000017\n",
      "6. 3369.6892143000005\n",
      "7. 3391.8590823999984\n",
      "0. 3391.9035183000015\n",
      "1. 3392.1856293999954\n",
      "3. 3395.5466480999967\n",
      "4. 3420.668362299999\n",
      "5. 3422.5766862999953\n",
      "6. 3427.4269287999996\n",
      "7. 3449.994175\n",
      "0. 3450.038235299995\n",
      "1. 3450.306391099999\n",
      "3. 3453.627666399996\n",
      "4. 3478.3643813000017\n",
      "5. 3480.4760578999994\n",
      "6. 3485.465376399996\n",
      "7. 3507.596948899998\n",
      "0. 3507.6413427000007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 3507.915203999997\n",
      "3. 3511.497409399999\n",
      "4. 3535.589625200002\n",
      "5. 3537.5789914999987\n",
      "6. 3542.453275699998\n",
      "7. 3564.551000499996\n",
      "0. 3564.595456999996\n",
      "1. 3564.8645349\n",
      "3. 3568.2096644999983\n",
      "4. 3593.0025793999957\n",
      "5. 3594.872773999996\n",
      "6. 3599.580565999997\n",
      "7. 3621.8464823999966\n",
      "0. 3621.890795899999\n",
      "1. 3622.1591298999992\n",
      "3. 3625.6901509999952\n",
      "4. 3649.9896051999967\n",
      "5. 3652.1246209999954\n",
      "6. 3657.2277004999996\n",
      "7. 3679.3372363000017\n",
      "0. 3679.382724999996\n",
      "1. 3679.655423199998\n",
      "3. 3682.979513500002\n",
      "4. 3707.4961781999955\n",
      "5. 3709.4439646999963\n",
      "6. 3714.327894199996\n",
      "7. 3736.8464760999996\n",
      "0. 3736.8911725\n",
      "1. 3737.157319699996\n",
      "3. 3740.4528417999973\n",
      "4. 3766.773208300001\n",
      "5. 3769.5699892999983\n",
      "6. 3774.6741755999974\n",
      "7. 3798.169173000002\n",
      "0. 3798.213574100002\n",
      "1. 3798.4918158\n",
      "3. 3801.9315408999973\n",
      "4. 3826.0784617000027\n",
      "5. 3828.1807235000015\n",
      "6. 3833.1076933000004\n",
      "7. 3856.1051675000053\n",
      "0. 3856.149550300004\n",
      "1. 3856.426139599993\n",
      "3. 3859.881185899998\n",
      "4. 3885.0342935999943\n",
      "5. 3887.0458877999918\n",
      "6. 3891.8525467\n",
      "7. 3914.348515000005\n",
      "0. 3914.3938134999917\n",
      "1. 3914.664166699993\n",
      "3. 3918.1430069999915\n",
      "4. 3942.701497400005\n",
      "5. 3944.719584099992\n",
      "6. 3949.4296831999964\n",
      "7. 3971.3580652999954\n",
      "0. 3971.4022879999975\n",
      "1. 3971.678961400001\n",
      "3. 3975.3896911000047\n",
      "4. 3999.852718299997\n",
      "5. 4002.0830111999967\n",
      "6. 4007.1753038000024\n",
      "7. 4030.185540999999\n",
      "0. 4030.2307253000035\n",
      "1. 4030.5084900999937\n",
      "3. 4033.781977199993\n",
      "4. 4058.6007873999915\n",
      "5. 4060.5797120999923\n",
      "6. 4065.225475200001\n",
      "7. 4088.111952799998\n",
      "0. 4088.157778199995\n",
      "1. 4088.428622899999\n",
      "3. 4091.7940869000013\n",
      "4. 4115.852290099996\n",
      "5. 4118.233522900002\n",
      "6. 4122.970514400004\n",
      "7. 4145.246622799998\n",
      "0. 4145.291180600005\n",
      "1. 4145.558202999993\n",
      "3. 4149.098981200004\n",
      "4. 4173.289514100004\n",
      "5. 4175.291726800002\n",
      "6. 4179.988603199992\n",
      "7. 4202.553916499994\n",
      "0. 4202.599023899995\n",
      "1. 4202.872177500001\n",
      "3. 4206.276226699993\n",
      "4. 4230.892411199995\n",
      "5. 4232.817821200006\n",
      "6. 4237.9097192000045\n",
      "7. 4259.847123400003\n",
      "0. 4259.891919699992\n",
      "1. 4260.2460779999965\n",
      "3. 4263.901300199999\n",
      "4. 4287.894393299997\n",
      "5. 4289.824116899996\n",
      "6. 4295.040721700003\n",
      "7. 4317.228972800003\n",
      "0. 4317.273235100001\n",
      "1. 4317.540032199991\n",
      "3. 4320.837839\n",
      "4. 4345.262038999994\n",
      "5. 4347.239010699996\n",
      "6. 4351.853702299995\n",
      "7. 4374.412342099997\n",
      "0. 4374.457034099993\n",
      "1. 4374.730101199995\n",
      "3. 4378.053989799999\n",
      "4. 4402.1632369\n",
      "5. 4404.489759100004\n",
      "6. 4409.320424999998\n",
      "7. 4431.328421800004\n",
      "0. 4431.372805299994\n",
      "1. 4431.638863500004\n",
      "3. 4435.054945600001\n",
      "4. 4459.613038900003\n",
      "5. 4461.527304799994\n",
      "6. 4466.175197599994\n",
      "7. 4488.534295699996\n",
      "0. 4488.577978100002\n",
      "1. 4488.844963199997\n",
      "3. 4492.341643299995\n",
      "4. 4516.848329400003\n",
      "5. 4519.018842899997\n",
      "6. 4523.783970799996\n",
      "7. 4546.023719999997\n",
      "0. 4546.069205799999\n",
      "1. 4546.335445000004\n",
      "3. 4549.947621999992\n",
      "4. 4574.304273500005\n",
      "5. 4576.299309199996\n",
      "6. 4581.240433500003\n",
      "7. 4603.3659014\n",
      "0. 4603.410738299994\n",
      "1. 4603.679883299992\n",
      "3. 4607.160679599998\n",
      "4. 4631.9161580999935\n",
      "5. 4633.825646600002\n",
      "6. 4638.3352571\n",
      "7. 4660.150561000002\n",
      "0. 4660.1950144\n",
      "1. 4660.6663523000025\n",
      "3. 4664.229992099994\n",
      "4. 4688.121791600002\n",
      "5. 4690.2602415\n",
      "6. 4695.338487699992\n",
      "7. 4717.550822599995\n",
      "0. 4717.5956762999995\n",
      "1. 4717.872021599993\n",
      "3. 4721.270912299995\n",
      "4. 4746.878656600005\n",
      "5. 4748.792862300004\n",
      "6. 4753.394513799998\n",
      "7. 4783.3417721000005\n",
      "0. 4783.402331999998\n",
      "1. 4783.735986300002\n",
      "3. 4787.664543799998\n",
      "4. 4823.800715499994\n",
      "5. 4826.309379700004\n",
      "6. 4831.9458157000045\n",
      "7. 4863.7129785\n",
      "0. 4863.767412300003\n",
      "1. 4864.057749500003\n",
      "3. 4868.065695700003\n",
      "4. 4908.344996899992\n",
      "5. 4911.828592499995\n",
      "6. 4919.285185799992\n",
      "7. 4959.715018300005\n",
      "0. 4959.763456600005\n",
      "1. 4960.074092700001\n",
      "3. 4963.6263172000035\n",
      "4. 4990.588002100005\n",
      "5. 4992.827617499992\n",
      "6. 4997.893664200004\n",
      "7. 5021.1078954999975\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "# Import annotated tweets dataset\n",
    "columns =  [\"Sentiment\", \"ID\", \"Date\", \"Flag\", \"User\", \"Text\"]\n",
    "encoding = \"ISO-8859-1\"\n",
    "tagg_tweets = pd.read_csv(r'C:\\Users\\muell\\Desktop\\Thesis\\MA\\Data_Tagged\\training_1600000.csv', \\\n",
    "                          encoding=encoding , names=columns)\n",
    "\n",
    "# Only keep relevant data\n",
    "tagg_tweets = tagg_tweets[['Sentiment','Text']]\n",
    "\n",
    "# Replace positive sentiment dummy (i.e. =4) with 1. Negative sentiment = 0\n",
    "tagg_tweets['Sentiment'] = tagg_tweets['Sentiment'].replace(4,1)\n",
    "\n",
    "# Go through same pre-processing as above\n",
    "total_rows = tagg_tweets.Text.count()\n",
    "N_splits = int(round(total_rows/20000))\n",
    "tagg_tweets_arr = np.array_split(tagg_tweets,N_splits)\n",
    "\n",
    "def large_pre_proc(arr_num):\n",
    "    tagg_tweets = tagg_tweets_arr[arr_num]\n",
    "    # to account for shift in loop below\n",
    "    sum_shift = 0\n",
    "    for ab in range(0,arr_num):\n",
    "        sum_shift = len(tagg_tweets_arr[ab]) + sum_shift\n",
    "\n",
    "    print('0. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "    # Converts all letters into lower case\n",
    "    tagg_tweets['Text'] = tagg_tweets['Text'].str.lower()\n",
    "\n",
    "    import re\n",
    "    # Delete all http & https URLs\n",
    "    tagg_tweets['Text'] = [re.sub(r\"http\\S+|rhttps\\S+|rwww\\S+\", \"\", e) for e in tagg_tweets['Text']]\n",
    "\n",
    "    # Delete all User Mentions\n",
    "    tagg_tweets['Text'] = [re.sub(r\"@\\S+\", \"\", e) for e in tagg_tweets['Text']]\n",
    "\n",
    "    # Remove all hashtags\n",
    "    tagg_tweets['Text'] = [re.sub(r'#([^\\s]+)', r'\\1', e) for e in tagg_tweets['Text']]\n",
    "\n",
    "    # Count number of $ in each Tweet to use as a proxy for tweet specificity (i.e. number of \n",
    "    # stock tickers referenced in each tweet) later on\n",
    "    tagg_tweets['N_Ticker'] = [int(e.count(\"$\")) for e in tagg_tweets['Text']]\n",
    "\n",
    "    # Remove all Cashtags \n",
    "    tagg_tweets['Text'] = [re.sub(r\"\\$\\S+\", '', e) for e in tagg_tweets['Text']]\n",
    "\n",
    "    # Mitigate elongated words, by detecting positions where character is repeated 3+ times and reduced to 1 time (approximation)\n",
    "    tagg_tweets['Text'] = [re.sub(r'((\\w)\\2{2,})', r'\\2', e) for e in tagg_tweets['Text']]\n",
    "    # Apply spellchecker to correct elongated words (and increase performance of above approximation)\n",
    "    # Extremely time intense (ignore for now)\n",
    "    # from autocorrect import Speller\n",
    "    # check = Speller(lang='en')\n",
    "    print('1. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "    import nltk\n",
    "    # list of english words, used to later remove non-english words\n",
    "    words = set(nltk.corpus.words.words())\n",
    "\n",
    "    # nltk.download('punkt')\n",
    "    from nltk.corpus import stopwords\n",
    "    # nltk.download('stopwords')\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    # Lemmatize the words ( preferred relative to stemming )\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Tokenize the respective tweets\n",
    "    tweet_tokens = [word_tokenize(e) for e in tagg_tweets['Text']]\n",
    "    print('3. ' + str(timeit.default_timer() - start_time))\n",
    "    for i in range(0,len(tweet_tokens)):\n",
    "        for p in ['a','s','r','n','v']:\n",
    "            tweet_tokens[i] = [lemmatizer.lemmatize(w, pos=p) for w in tweet_tokens[i]] \n",
    "            # join list of words back to a sentence    \n",
    "        # result.loc[i, 'Text'] = ' '.join(tweet_tokens[i])\n",
    "        # Delete all words which are non-english to reduce dimensionality of matrix later on\n",
    "        tagg_tweets.loc[(i+sum_shift), 'Text'] = [\" \".join(w for w in nltk.wordpunct_tokenize(str(tweet_tokens[i])) if w in words or not w.isalpha())]\n",
    "    print('4. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "    # Negation handling:\n",
    "    # Detects negations and transforms negated words into 'not_' form\n",
    "    def negate_sequence(text):\n",
    "        negation = False\n",
    "        delims = \"?.,!:;\"\n",
    "        result = []\n",
    "        words = text.split()\n",
    "        prev = None\n",
    "        pprev = None\n",
    "        for word in words:\n",
    "            stripped = word.strip(delims).lower()\n",
    "            negated = \"not_\" + stripped if negation else stripped\n",
    "            result.append(negated)\n",
    "            prev = negated\n",
    "\n",
    "            if any(neg in word for neg in [ 'barely', 'cannot',  'hardly', 'lack', 'lacking', 'lacks', 'neither', 'never', \\\n",
    "                                           'no one', 'nobody', 'none',  'nothing', 'nowhere',  'scarcely', \\\n",
    "                                           'without', ' not ', \"n't\", \" no \"]):\n",
    "                negation = not negation\n",
    "\n",
    "            if any(c in word for c in delims):\n",
    "                negation = False\n",
    "\n",
    "        return result\n",
    "    # Application of def negate_sequence for negation handling\n",
    "    tagg_tweets_temp = tagg_tweets['Text'].apply(negate_sequence)            \n",
    "    tagg_tweets[\"Text\"] = list(\" \".join(e) for e in tagg_tweets_temp)\n",
    "    print('5. ' + str(timeit.default_timer() - start_time))\n",
    "    # Remove stopwords\n",
    "    # Tokenize the respective tweets\n",
    "    tweet_tokens = [word_tokenize(e) for e in tagg_tweets['Text']]\n",
    "    print('6. ' + str(timeit.default_timer() - start_time))\n",
    "    # Define the list of stopwords which will be removed\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    #remove_list = []\n",
    "    # all_stopwords = [word for word in all_stopwords if word not in remove_list]\n",
    "    # Add additional stop words to be removed from texts\n",
    "    sw_list = ['amp', 'aint', \"ain't\", 'barely', 'cannot ', 'cant', \"can't\", 'couldnt', \"couldn't\", 'darent', \"daren't\", 'didnt', \\\n",
    "    \"didn't\", \"doesnt\", \"doesn't\", 'dont', \"don't\", 'hadnt', \"hadn't\", 'hardly', 'hasnt', \"hasn't\", 'havent', \\\n",
    "    \"haven't\", 'isnt', \"isn't\", 'lack', 'lacking', 'lacks', 'mightnt', \"mightn't\", 'mustnt', \"mustn't\", \\\n",
    "    'neednt', \"needn't\", 'neither', 'never', 'no one', 'nobody', 'none', 'nor',  'nothing', \\\n",
    "    'nowhere', 'oughtnt', \"oughtn't\", 'scarcely', 'shant', \"shan't\", 'shouldnt', \"shouldn't\", 'wasnt', \\\n",
    "    \"wasn't\", 'without', 'wont', \"won't\", 'wouldnt', \"wouldn't\"]\n",
    "    all_stopwords.extend(sw_list)\n",
    "#        remove_list = ['not']\n",
    "#        all_stopwords = [word for word in all_stopwords if word not in remove_list]\n",
    "    for i in range(0,len(tweet_tokens)):        \n",
    "        # Delete all stopwords\n",
    "        tagg_tweets.loc[(i+sum_shift), 'Text'] = [\" \".join(word for word in tweet_tokens[i] if word not in all_stopwords)]        \n",
    "\n",
    "\n",
    "    # Remove all punctuations and all special chars and replace with space\n",
    "    import string\n",
    "    tagg_tweets['Text'] = tagg_tweets['Text'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "    # Remove all numbers\n",
    "    tagg_tweets['Text'] = [re.sub(\"\\d+\", \"\", e) for e in tagg_tweets['Text']]\n",
    "\n",
    "    # Remove short words with 2 chars or less\n",
    "    tagg_tweets['Text'] = [re.sub(r'\\b\\w{1,2}\\b', '', e) for e in tagg_tweets['Text']]\n",
    "\n",
    "    # Replace multiple whitespaces with single whitespace\n",
    "    tagg_tweets['Text'] = [' '.join(str(e).split()) for e in tagg_tweets['Text']]\n",
    "    print('7. ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "    return tagg_tweets\n",
    "\n",
    "tagg_tweets = list(map(large_pre_proc,list(range(0, (N_splits)))))\n",
    "\n",
    "# Merge file\n",
    "new_tagg_tweets = pd.DataFrame(tagg_tweets[0])\n",
    "for x in range (1,N_splits):\n",
    "    new_tagg_tweets = new_tagg_tweets.append(pd.DataFrame(tagg_tweets[x]), ignore_index=True) \n",
    "new_tagg_tweets = new_tagg_tweets[['Sentiment','Text']]\n",
    "new_tagg_tweets.to_csv(r'C:\\Users\\muell\\Desktop\\Thesis\\MA\\Data_Tagged\\training_1600000_clean.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Select most sentiment-rich words and allocate pos/neg sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in saved csv\n",
    "tagg_tweets = pd.read_csv(r'C:\\Users\\muell\\Desktop\\Thesis\\MA\\Data_Tagged\\training_1600000_clean.csv')\n",
    "# Kick out all unrequired columns (existed in testing phase of code)\n",
    "tagg_tweets = tagg_tweets[['Sentiment','Text']]\n",
    "# Fill cells without any string with na\n",
    "tagg_tweets['Text'] = tagg_tweets['Text'].fillna('')\n",
    "# Remove all tweets with negations (i.e. 'not_'). Avoids wrong handling of negations and results in sufficient df with \n",
    "# 1'428'989 Observations/rows/tweets (out of 1.6M)\n",
    "tagg_tweets = tagg_tweets[~tagg_tweets.Text.str.contains('not_')]\n",
    "# Split dataset into negative and positive, such that memory of laptop can handle subsequent operations\n",
    "tagg_tweets_neg = tagg_tweets[tagg_tweets['Sentiment'] == 0]\n",
    "tagg_tweets_pos = tagg_tweets[tagg_tweets['Sentiment'] == 1]\n",
    "# Apply TF-IDF weighting to select 2000 most highly weighted features per sentiment pole (i.e. max_features=1000), with \n",
    "# minimum of 100 occurences (i.e. min_df=100) and maximum occurence in 15% of tweets (i.e. max_df=0.15)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from nltk.corpus import stopwords\n",
    "vectorizer = TfidfVectorizer(max_features=2000, min_df=100, max_df=0.15, stop_words=stopwords.words('english'))  \n",
    "# Apply tf-idf weighting to positive sample\n",
    "tf_idf_pos = vectorizer.fit_transform(tagg_tweets_pos.Text)\n",
    "# Extract the 1000 feature names from prior tf-idf weighting\n",
    "feature_names_pos['Features']  = pd.DataFrame(np.array(vectorizer.get_feature_names()))\n",
    "# Same for negative sentiment\n",
    "tf_idf_neg = vectorizer.fit_transform(tagg_tweets_neg.Text)\n",
    "feature_names_neg['Features'] = pd.DataFrame(np.array(vectorizer.get_feature_names()))\n",
    "# Create one df with all 1k negative + positive features and remove duplicates\n",
    "feature_names = feature_names_pos.head(n=1000).append(feature_names_neg.head(n=1000)).drop_duplicates()\n",
    "# Kick out all unrequired columns (existed in testing phase of code)\n",
    "feature_names = feature_names[['Features']]\n",
    "# Count all positive occurences of each word on the feature_names list\n",
    "count_pos = \\\n",
    "    pd.Series({w: tagg_tweets_pos['Text'].str.contains(w, case=False).sum() for w in list(feature_names['Features'])})\n",
    "# Store the resulting series in a df\n",
    "count_pos_df = pd.DataFrame({'Features':count_pos.index, 'Count_pos':count_pos.values})\n",
    "# Left join the count of positive occurences to the main df feature_names\n",
    "feature_names = pd.merge(feature_names,count_pos_df,on='Features',how='left')\n",
    "# Same for negative word occurences\n",
    "count_neg = \\\n",
    "    pd.Series({w: tagg_tweets_neg['Text'].str.contains(w, case=False).sum() for w in list(feature_names['Features'])})\n",
    "count_neg_df = pd.DataFrame({'Features':count_neg.index, 'Count_neg':count_neg.values})\n",
    "feature_names = pd.merge(feature_names,count_neg_df,on='Features',how='left')\n",
    "# Create column with percentage of positive vs overall occurences of the respective word\n",
    "feature_names['Perc_pos'] = feature_names['Count_pos']/(feature_names['Count_pos'] + feature_names['Count_neg'])\n",
    "# Create column with respective sentiment, default is 0\n",
    "feature_names['Sentiment'] = 0\n",
    "# If 75% or more of occurences are in positive context than word is attributed positive sentiment\n",
    "feature_names.loc[feature_names['Perc_pos'] >= 0.75, 'Sentiment'] = 1\n",
    "# Same for negative\n",
    "feature_names.loc[feature_names['Perc_pos'] <= 0.25, 'Sentiment'] = -1\n",
    "# Drop all words with neutral sentiment (i.e. feature_names['Sentiment'] == 0)\n",
    "feature_names = feature_names[~(feature_names['Sentiment'] == 0)]\n",
    "# write to csv to join in excel with other lexica and read-in later on\n",
    "feature_names.to_csv(r'C:\\Users\\muell\\Desktop\\Thesis\\MA\\Data_Tagged\\feature_names.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad-hoc analysis tries (re-use might be advantageous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = ' '.join(word for word in result.Text)\n",
    "print (\"There are {} words in the combination of all review.\".format(len(text)))\n",
    "\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dummy = 1\n",
    "if Dummy == 1:\n",
    "    stock_adj = \"AAPL_adj\"\n",
    "    conn = sqlite3.connect(r\"C:\\Users\\muell\\Desktop\\Thesis\\MA\\SQL_DB\\Tw_DB.db\")\n",
    "    c = conn.cursor()\n",
    "    # Select entire table of respective stock ticker\n",
    "    query = '''select ID, Text from {} '''.format(stock_adj)\n",
    "    c.execute(query)\n",
    "    # Convert to pandas DF format\n",
    "    result = pd.DataFrame(c.fetchall())\n",
    "    # Rename all columns accordingly\n",
    "    result.rename(columns={0: 'ID', 1: 'Text'}, inplace=True)\n",
    "\n",
    "else:\n",
    "    start_time = timeit.default_timer()\n",
    "    for a in range(0,len(stocklist)):\n",
    "\n",
    "        stock_adj = stocklist[a] + '_adj'\n",
    "        stock = stocklist[a]\n",
    "        conn = sqlite3.connect(r\"C:\\Users\\muell\\Desktop\\Thesis\\MA\\SQL_DB\\Tw_DB.db\")\n",
    "        c = conn.cursor()\n",
    "        # Select entire table of respective stock ticker\n",
    "        query = '''select ID, Text from {} '''.format(stock_adj)\n",
    "        c.execute(query)\n",
    "        # Convert to pandas DF format\n",
    "        if a == 0:\n",
    "            result = pd.DataFrame(c.fetchall())\n",
    "            # Rename all columns accordingly\n",
    "            result.rename(columns={0: 'ID', 1: 'Text'}, inplace=True)\n",
    "        else: \n",
    "            result_add = pd.DataFrame(c.fetchall())\n",
    "            # Rename all columns accordingly\n",
    "            result_add.rename(columns={0: 'ID', 1: 'Text'}, inplace=True)\n",
    "            result = pd.concat([result, result_add])\n",
    "    print(result)  \n",
    "    print('Final: ' + str(timeit.default_timer() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320768\n",
      "1: 0.003376300010131672\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 17.9 GiB for an array with shape (800000, 3000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-ba173dd165ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtf_idf_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_rand_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_idf_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 17.9 GiB for an array with shape (800000, 3000) and data type float64"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "result_rand = result.sample(frac=1)\n",
    "# Enough memory for 800k x 2k tf-idf mat\n",
    "# len_samp = round(len(result_rand)/4)\n",
    "# print(len_samp)\n",
    "# result_rand_1 = result_rand[:len_samp]\n",
    "# result_rand_2 = result_rand[len_samp:]\n",
    "print('1: ' + str(timeit.default_timer() - start_time))\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "vectorizer = TfidfVectorizer(max_features=2000, min_df=100, max_df=0.15, stop_words=stopwords.words('english'))  \n",
    "tf_idf_1 = pd.DataFrame(vectorizer.fit_transform(result_rand_1.Text).toarray())\n",
    "print(tf_idf_1)\n",
    "    \n",
    "feature_names = pd.DataFrame(np.array(vectorizer.get_feature_names()))\n",
    "print(feature_names)\n",
    "print('Final: ' + str(timeit.default_timer() - start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
